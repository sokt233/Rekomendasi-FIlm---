# -*- coding: utf-8 -*-
"""Submission

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QtRy7hOftr10mI4wIL_scnOBMD6G044b

# Sistem Rekomendasi Film - Gratia Yudika Morado Silalahi

# Import Library yang Digunakan
"""

import pandas as pd  # manipulasi data
import matplotlib.pyplot as plt  # visualisasi data
import seaborn as sns  # visualisasi statistik
import numpy as np  # operasi numerik

import tensorflow as tf  # library deep learning
from tensorflow import keras  # API untuk model
from tensorflow.keras import layers  # komponen layer jaringan

from sklearn.feature_extraction.text import TfidfVectorizer  # ubah teks ke vektor TF-IDF
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel  # hitung kemiripan antar item

"""# Data Understanding

## Load Dataset
"""

# Konversi data film ke dalam DataFrame
film_df = pd.read_csv('movies.csv', on_bad_lines='skip')
film_df.head()

# konversi data rating ke dalam Dataframe
rating_df = pd.read_csv('ratings.csv', on_bad_lines='skip')
rating_df.head()

# Pemeriksaan jumlah film
print('Jumlah film: ',len(film_df.movieId.unique()))
print('Jumlah rating: ',len(rating_df.userId.unique()))

"""Informasi pemahaman data
Dataset yang digunakan diambil dari repository kaggle dengan informasi berikut.
- Nama file: Movies & Ratings for Recommendation System
- Author: Nicoleta Cilibiu
- Tautan: https://www.kaggle.com/datasets/nicoletacilibiu/movies-and-ratings-for-recommendation-system

# Exploratory Data Analysis

## Deskripsi Variabel
"""

print("Informasi DataFrame Film")
print("=" * 35)
film_df.info()
print("\n")  # baris kosong untuk pemisah

print("Informasi DataFrame Rating")
print("=" * 38)
rating_df.info()

"""Dataframe film memuat 3 variabel di dalamnya, yakni dengan 1 integer pada movieid dan 2 teks pada title dan genres. Pada dataframe rating, variabel yang dimuat yakni 3 integer: userID, movieID, dan timestamp serta float untuk rating.

Sejauh ini diketahui tidak ada missing value dalam dataset, namun untuk outlier dan duplikasi akan diperiksa selanjutnya.

## Penggambaran data
"""

# Deskripsi Dataset Film
print("Penggambaran DataFrame Film")
print("=" * 20)
film_df.describe()

"""- ID film dimulai dari 1 hingga 193.609, menunjukkan bahwa movieId tidak berurutan dan kemungkinan besar mengandung celah (gap) antar ID.
- Nilai tengah (median) movieId adalah 7.300, tetapi nilai maksimum jauh lebih tinggi, menandakan distribusi miring ke kanan (right-skewed) â€” sebagian besar film memiliki movieId lebih kecil dibanding outlier di ujung atas.
"""

# Deskripsi Dataset Rating
print("Penggambaran DataFrame Rating")
print("=" * 63)
rating_df.describe()

"""- Jumlah pengguna: antara userId 1 hingga 610 â€” menandakan ada 610 pengguna unik.
- Nilai rating berada pada rentang 0.5 hingga 5.0, dengan rata-rata ~3.5, menunjukkan bahwa pengguna cenderung memberi rating netral hingga positif.
- Variasi rating (std = 1.04) cukup rendah â†’ mayoritas rating terpusat di sekitar nilai tengah.
- Timestamp menunjukkan rentang waktu yang luas: dari sekitar 1996 hingga 2018, menunjukkan data historis yang panjang, cocok untuk analisis temporal (misalnya, perubahan preferensi film dari waktu ke waktu).

## Pemeriksaan missing value, data outlier dan duplikat

### Missing Value
"""

print("Pemeriksaan Nilai yang hilang")
Film_hilang = film_df.isnull().sum()
print('Film: ', Film_hilang)

missing_values_rating = rating_df.isnull().sum()
print('Rating: ', missing_values_rating)

"""Tidak ada data yang hilang

### Outlier
"""

# Visualisasi histogram
cols = ['userId', 'movieId', 'rating']

# Set gaya visual
sns.set(style="whitegrid")

# Warna berbeda untuk tiap kolom
colors = ['#4C72B0', '#55A868', '#C44E52']

# Buat subplot 1 baris 3 kolom
fig, axes = plt.subplots(1, 3, figsize=(20, 5))

# Loop setiap kolom dan buat histogram-nya
for i, col in enumerate(cols):
    bins = 30 if col != 'rating' else 10  # rating lebih sempit skalanya
    sns.histplot(
        data=rating_df,
        x=col,
        bins=bins,
        kde=True,
        ax=axes[i],
        color=colors[i],
        edgecolor='black'
    )
    axes[i].set_title(f'Distribusi {col}', fontsize=14, fontweight='bold')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frekuensi')
    axes[i].grid(True, linestyle='--', alpha=0.5)

# Tata letak agar tidak tumpang tindih
plt.tight_layout()
plt.show()

"""Dari yang kita lihat, data moveID sangat tidak merata penyebarannya apalagi dengan visualisasi yang menunjukkan data sangat right skewed. Tapi pengolahan tetap dapat dilanjutkan karena tidak ada bukti yang kuat bahwa data tersebut memiliki outlier pada datasetnya.

### Duplikat
"""

print("Jumlah data yang duplikat")

# data duplikat Film
jumlah_duplikat_film = film_df.duplicated().sum()
print('Film: ', jumlah_duplikat_film)

# data duplikat rating
jumlah_duplikat_rating = rating_df.duplicated().sum()
print('Rating', jumlah_duplikat_rating)

"""Karena tidak ada data yang duplikat, maka akan dicoba untuk memeriksa dari id nya."""

print("Jumlah data yang unik")

# Jumlah data unik di kolom title
unique_titles = film_df['title'].nunique()
print('Judul :', unique_titles)

# Jumlah data unik di kolom movieId
unique_movieIds = film_df['movieId'].nunique()
print('movieId:', unique_movieIds)

"""Karena jumlahnya berbeda maka akan diperiksa berdasarkan movieID setiap judul film yang terkait."""

duplicates = film_df.groupby('title')['movieId'].nunique()
duplicates = duplicates[duplicates > 1]
print(duplicates)

"""Dan dugaan benar, secara pengolahan awal tidak terdapat data yang duplikat. Namun jika melihat kecocokan ID, maka beberapa film ditemukan dengan ID yang tidaklah sama. Sehingga beberapa film pun terindikasi duplikat.

## Analisis data tunggal - Univariate

### Analisis Distribusi Genre Film
"""

# Mengurai genre menjadi satu per baris
all_genres = film_df['genres'].str.split('|').explode()

# Hitung jumlah genre unik
unique_genres = all_genres.unique()
print(f'ðŸ“Š Jumlah genre: {len(unique_genres)}\n')

# Hitung jumlah film per genre
genre_counts = all_genres.value_counts()

# Visualisasi dengan barplot dan anotasi jumlah film
plt.figure(figsize=(12, 6))
ax = sns.barplot(x=genre_counts.values, y=genre_counts.index, palette='viridis')

plt.title('Distribusi Genre', fontsize=14, fontweight='bold')
plt.xlabel('Jumlah Film')
plt.ylabel('Genre')

# Tambahkan angka jumlah film di samping bar
for i, v in enumerate(genre_counts.values):
    ax.text(v + 5, i, str(v), color='black', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

"""Berdasarkan eksplorasi awal terhadap kolom genres, ditemukan bahwa dataset ini memuat sebanyak 20 genre unik. Genre Drama menjadi yang paling dominan dalam koleksi film yang tersedia, disusul oleh Comedy dan Thriller sebagai genre terbanyak berikutnya. Di sisi lain, terdapat beberapa genre yang jarang muncul, seperti Film-Noir dan Western, yang hanya diwakili oleh sedikit film.

Menariknya, terdapat 34 film yang tidak memiliki informasi genre sama sekali. Karena proyek ini akan membangun sistem rekomendasi berbasis konten (content-based filtering) yang bergantung pada atribut genre sebagai fitur utama, maka film-film tanpa genre ini sebaiknya dikeluarkan dari analisis untuk menjaga konsistensi dan keakuratan hasil model.

### Analisis Distribusi Rating Pengguna
"""

# Set gaya visual
sns.set(style="whitegrid")

# Ukuran figure
plt.figure(figsize=(10, 6))

# Plot distribusi rating
ax = sns.countplot(x='rating', data=rating_df, palette='magma')

# Tambahkan judul dan label sumbu
plt.title('Distribusi Rating Film oleh Pengguna', fontsize=14, fontweight='bold')
plt.xlabel('Rating')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=0)

# Tambahkan angka jumlah di atas setiap bar
for p in ax.patches:
    height = int(p.get_height())
    ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom', fontsize=10, color='black')

# Tata letak agar rapi
plt.tight_layout()
plt.show()

"""Mayoritas pengguna cenderung memberikan penilaian di kisaran **moderat**, yakni antara **3.0 hingga 4.0**, dengan **rating 4.0** menjadi yang paling sering diberikan. Sementara itu, **rating sempurna (5.0)** juga cukup banyak muncul, menunjukkan bahwa pengguna relatif sering memberikan apresiasi tinggi, terutama jika dibandingkan dengan frekuensi rating rendah yang lebih jarang diberikan.

### Distribusi Film Dengan Akumulasi Rating - Top 20 rating tertinggi
"""

# Hitung jumlah rating per film
rating_counts = rating_df.groupby('movieId').size().reset_index(name='rating_count')

# Gabungkan dengan movies_df agar dapat judul film
movie_rating_counts = rating_counts.merge(film_df[['movieId', 'title']], on='movieId')

# Urutkan berdasarkan rating_count terbesar dan ambil 20 teratas
top20_most_rated = movie_rating_counts.sort_values(by='rating_count', ascending=False).head(20)

# Visualisasi dengan bar chart horizontal seaborn
plt.figure(figsize=(14, 8))
ax = sns.barplot(x='rating_count', y='title', data=top20_most_rated, palette='Blues_d')

plt.title('20 Film dengan Jumlah Rating Terbanyak', fontsize=16, fontweight='bold')
plt.xlabel('Jumlah Rating', fontsize=12)
plt.ylabel('Judul Film', fontsize=12)

# Tambahkan anotasi jumlah rating di ujung bar
for i, count in enumerate(top20_most_rated['rating_count']):
    ax.text(count + 5, i, str(count), va='center', fontsize=10)

plt.tight_layout()
plt.show()

"""Tahun 1990-an menjadi tahun terbanyak dari 20 film terbaik yang ada. Faktor bahwa di zaman itu hiburan belum terlalu bervariasi menjadi salah 1 faktor terkuat dari analisis tersebut

# Data Preparation

## Data Film

Hapus film dengan genre 'no genres listed'
"""

# Hapus baris yang kolom 'genres' berisi '(no genres listed)'
fix_movie = film_df[film_df['genres'] != '(no genres listed)'].copy()

# Cek hasil
print(f"Sebelum: {len(film_df)} baris")
print(f"Setelah: {len(fix_movie)} baris")

"""Hapus film dengan judul yang duplikat"""

fix_movie = fix_movie.drop_duplicates(subset='title', keep='first')
duplicates = fix_movie.groupby('title')['movieId'].nunique()
duplicates = duplicates[duplicates > 1]
print(duplicates)

"""Film dan rating digabungkan"""

merged_df = pd.merge(
    rating_df,
    film_df,
    on='movieId',
    how='left'
    )
merged_df.head()

"""## Data Preparation untuk algoritma Content-based Filtering"""

# mengonversi setiap data series 'movieId' menjadi bentuk list
moviesId = fix_movie['movieId'].tolist()

# mengonversi data 'title' menjadi bentuk list
title = fix_movie['title'].tolist()

# mengonversi data 'genres' menjadi bentuk list
genres = fix_movie['genres'].tolist()

print(len(moviesId))
print(len(title))
print(len(genres))

# Membuat dictionary untuk data â€˜moviesIdâ€™, â€˜titleâ€™, dan â€˜genresâ€™
movies_new = pd.DataFrame({
    'moviesId': moviesId,
    'title': title,
    'genres': genres
})
movies_new

"""Insight penting dalam pengolahan data film adalah pembuatan dictionary yang berfungsi untuk memetakan pasangan key-value antara movieId, title, dan genres. Dengan menggunakan dictionary ini, setiap movieId unik akan menjadi kunci (key) yang terhubung langsung dengan nilai (value) berupa judul film (title) dan kategori genre (genres)"""

# membuat salinan dari data movies
data_movies = movies_new

"""### TF_IDF Vectorizer"""

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

tfidf_matrix = tfidf.fit_transform(data_movies['genres'])

tfidf.get_feature_names_out()

"""Insight penting dalam pemrosesan data genre film adalah mengubah teks genre menjadi representasi numerik dengan menggunakan metode TF-IDF (Term Frequency-Inverse Document Frequency). Proses ini mengkonversi setiap kumpulan genre, misalnya "Action|Adventure|Fantasy", menjadi sebuah vektor numerik yang merepresentasikan bobot masing-masing genre dalam konteks keseluruhan dataset."""

tfidf_matrix.shape

"""Konversi Sparse Matrix ke Dense Matrix"""

tfidf_matrix.todense()

"""Konversi matriks TF-IDF dari format sparse menjadi format dense dilakukan agar data dapat diproses dan ditampilkan secara lengkap dan mudah dipahami. Dengan format dense, seluruh elemen matriks tersedia secara eksplisit, sehingga memudahkan analisis lebih lanjut dan visualisasi fitur








"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data_movies['title']
).sample(21, axis=1).sample(10, axis=0)

"""## Data preparation untuk Collaborative Filtering

Salin pada dataframe baru data rating sebelumnya
"""

rating_new = rating_df.drop(columns=['timestamp'])
rating_new.head()

"""Penghapusan kolom timestamp dari dataset rating dilakukan karena kolom ini berisi informasi waktu rating yang dalam konteks pembangunan sistem rekomendasi dianggap tidak relevan. Hal ini karena rekomendasi fokus pada preferensi pengguna terhadap film sehingga data waktu tersebut dianggap tidak memberikan kontribusi signifikan terhadap akurasi model.

Data Encoding dilakukan untuk mengubah nilai asli dari userId dan movieId menjadi bentuk numerik yang terurut (dari 0
hingga N-1), agar dapat digunakan sebagai input dalam model machine learning seperti embedding layer pada collaborative
filtering.
"""

# Encoding data userId
userIds = rating_new['userId'].unique().tolist()
print('list userIds:', userIds)

user_to_user_encoded = {x: i for i, x in enumerate(userIds)}
print('encoded userIds : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(userIds)}
print('encoded angka ke userIds: ', user_encoded_to_user)

# encoding data movie
movieIds = rating_new['movieId'].unique().tolist()
print('list movieIds:', movieIds)

movie_to_movie_encoded = {x: i for i, x in enumerate(movieIds)}
print('encoded movieID : ', movie_to_movie_encoded)

movie_encoded_to_movie = {i: x for i, x in enumerate(movieIds)}
print('encoded angka ke movieID: ', movie_encoded_to_movie)

# menentukan jumlah user di dalam data rating
num_users = len(userIds)
print('jumlah user:', num_users)

# menentukan jumlah film di dalam data rating
num_movies = len(movieIds)
print('jumlah movie:', num_movies)

# konversi tipe data rating menjadi float32
rating_df['rating'] = rating_df['rating'].values.astype(np.float32)

# nilai minimum dan maksimum rating dari rating_new
min_rating = min(rating_new['rating'])
max_rating = max(rating_new['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movies, min_rating, max_rating
))

"""Kode ini menghitung jumlah pengguna dan film unik serta menentukan rentang nilai rating untuk mengatur ukuran embedding layer dan memastikan konsistensi tipe data rating. Informasi ini penting untuk preprocessing seperti normalisasi agar model rekomendasi dapat dilatih secara efektif dan menghasilkan prediksi yang akurat."""

# mengacak kolom rating
rating_new = rating_new.sample(frac=1, random_state=42)
rating_new

"""Model akan mempelajari data baru adalah tujuan utama dari pengacakan data, sehingga model yang dihasilkan akan menjadi lebih baik"""

# Mapping userID ke dataframe user
rating_new['user'] = rating_new['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe movie
rating_new['movie'] = rating_new['movieId'].map(movie_to_movie_encoded)

X = rating_new[['user', 'movie']]
y = rating_new['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * rating_new.shape[0])
X_train, X_val, y_train, y_val = (
    X[:train_indices],
    X[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(X, y)

"""Proses di atas adalah mendefinisikan kolom fitur dan target dengan jelas, serta melakukan pemisahan data menjadi dua bagian dengan rasio 80:20. Sebanyak 80% data digunakan untuk melatih model, sementara 20% sisanya dialokasikan untuk evaluasi performa model. Pendekatan ini memastikan model dapat belajar dari data yang cukup dan diuji secara objektif pada data yang belum pernah dilihat sebelumnya.

# Modelling

## Content Based Filtering

### Cosine Similarity
"""

# menggunakan subset 1000 film pertama untuk menghemat ram
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim.shape

"""Similarity antar film dihitung berdasarkan fitur TF-IDF yang diambil dari deskripsi atau metadata film untuk mengukur tingkat kemiripan antar judul. Namun, untuk mengurangi penggunaan memori dan mempercepat proses perhitungan, perhitungan similarity ini dibatasi hanya pada 1000 film pertama. Pembatasan ini dilakukan karena menghitung similarity untuk seluruh dataset yang besar dapat sangat memberatkan dan memperlambat proses komputasi secara signifikan.

"""

cosine_sim_df = pd.DataFrame(
    cosine_sim,
    index=data_movies['title'],
    columns=data_movies['title']
)

print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Cosine similarity sebagai dasar fungsi rekomendasi film
def movie_recommendations(title, similarity_data=cosine_sim_df, items=data_movies[['title', 'genres']], k=5):
    index = similarity_data.loc[:, title].to_numpy().argpartition(range(-1, -k, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(title, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Merancang model untuk melakukan fungsi rekomendasi Content-based-Filtering"""

data_movies[data_movies.title.eq('Toy Story (1995)')]

movie_recommendations('Toy Story (1995)')

"""Mensimulasikan model rekomendasi terhadap film Toy Story (1995), dan berhasil.

## Collaborative Filtering
"""

class RecommenderNet(tf.keras.Model):

    # Inisialisasi fungsi
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_movies = num_movies
        self.embedding_size = embedding_size

        # Embedding untuk pengguna
        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        # Embedding untuk film
        self.movie_embedding = layers.Embedding(
            input_dim=num_movies,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])

        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

        x = dot_user_movie + user_bias + movie_bias

        return tf.nn.sigmoid(x)

"""Membangun dan mengompilasi model collaborative filtering"""

model = RecommenderNet(num_users, num_movies, embedding_size=50)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=8,
    epochs=30,
    validation_data=(X_val, y_val)
)

"""Proses melatih model dengan epoch sebesar 30"""

# Membuat plot
plt.figure(figsize=(10, 6))
plt.plot(history.history['root_mean_squared_error'], label='RMSE - Train', color='mediumseagreen', linewidth=2)
plt.plot(history.history['val_root_mean_squared_error'], label='RMSE - Validation', color='darkorange', linestyle='--', linewidth=2)

# Menambahkan judul dan label sumbu
plt.title('Model Evaluation - RMSE per Epoch', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Root Mean Squared Error (RMSE)', fontsize=12)

# Menambahkan grid dan legenda
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(loc='upper right', fontsize=10)

# Menampilkan plot
plt.tight_layout()
plt.show()

"""Visualisasi grafik root mean square error"""

# Ambil satu userId secara acak dari data
user_id = merged_df['userId'].sample(1).iloc[0]

# Ambil daftar film yang sudah ditonton oleh user tersebut
movies_watched_by_user = merged_df[merged_df.userId == user_id]

# Ambil daftar film yang belum ditonton oleh user tersebut
movies_not_watched = merged_df[
    ~merged_df['movieId'].isin(movies_watched_by_user['movieId'].values)
][['movieId']].drop_duplicates()

# Filter hanya movieId yang terdapat dalam kamus encoding
movies_not_watched = movies_not_watched[
    movies_not_watched['movieId'].isin(movie_to_movie_encoded.keys())
]

# Encode movieId menjadi indeks numerik
movies_not_watched_encoded = [
    [movie_to_movie_encoded[movie_id]] for movie_id in movies_not_watched['movieId'].values
]

# Encode userId menjadi indeks numerik
user_encoded = user_to_user_encoded.get(user_id)

# Buat array pasangan userId dan setiap movieId yang belum ditonton
user_movie_array = np.hstack((
    np.array([[user_encoded]] * len(movies_not_watched_encoded)),
    np.array(movies_not_watched_encoded)
))

# Prediksi rating
rating = model.predict(user_movie_array).flatten()

# Ambil indeks top-10 prediksi tertinggi
top_indices = rating.argsort()[-10:][::-1]

# Ambil movieId dari encoded
recommended_movie_ids = [
    movie_encoded_to_movie[i[0]] for i in np.array(movies_not_watched_encoded)[top_indices]
]

# Ambil judul dan genre film dari movieId rekomendasi
recommended_movies = merged_df[merged_df['movieId'].isin(recommended_movie_ids)][['movieId', 'title', 'genres']].drop_duplicates()
print(recommended_movies)

"""Mensimulaskan rekomendasi 10 film terbaik untuk satu pengguna yang dipilih secara acak berdasarkan model collaborative filtering, dan berhasil.

# Evaluation

## Content Based Filtering - Precision

**Precision** adalah matriks evaluasi kinerja model sistem rekomendasi yang mengukur seberapa banyak rekomendasi yang diberikan oleh sistem benar-benar relevan atau sesuai dengan preferensi pengguna. Secara matematis :

$$
\text{Precision} = \frac{\text{Jumlah item relevan yang direkomendasikan}}{\text{Jumlah total item yang direkomendasikan}}
$$
---
"""

target_genres = set(['Adventure', 'Children', 'Fantasy'])

# mengambil rekomendasi
recommendations = movie_recommendations('Toy Story (1995)')

# Fungsi cek relevansi
def is_relevant(genre_str, target_genres):
    genres = set(genre_str.split('|'))
    return target_genres.issubset(genres)

# menghitung precision
total_recs = len(recommendations)
relevant_recs = recommendations['genres'].apply(lambda x: is_relevant(x, target_genres)).sum()

precision = relevant_recs / total_recs if total_recs > 0 else 0

print(f"Precision: {precision:.2f}")

"""Sistem rekomendasi yang dibangun menggunakan metode Content-Based Filtering berhasil mencapai nilai evaluasi precision sebesar 100%. Capaian ini menunjukkan bahwa algoritma mampu merekomendasikan film dengan tingkat akurasi yang sangat tinggi, khususnya dalam mencocokkan genre antara film yang direkomendasikan dengan preferensi pengguna. Hasil ini mencerminkan efektivitas pendekatan content-based dalam menghasilkan rekomendasi yang relevan dan sesuai dengan karakteristik konten film.

## Collaborative Filtering - RMSE

**Root Mean Squared Error (RMSE)** dipilih sebagai metrik evaluasi untukmodel collaborative filtering ini. RMSE memberikan gambaran seberapa besar rata-rata kesalahan prediksi model dibandingkan rating asli dari pengguna, dengan satuan yang sama seperti rating.

Rumus RMSE:

$$
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2}
$$

di mana:

- $y_i$ adalah rating asli,
- $\hat{y}_i$ adalah rating prediksi,
- $N$ adalah jumlah data.
"""

# Plot RMSE
plt.figure(figsize=(10, 6))
plt.plot(history.history['root_mean_squared_error'], label='RMSE - Train', color='royalblue', linewidth=2)
plt.plot(history.history['val_root_mean_squared_error'], label='RMSE - Validation', color='crimson', linestyle='--', linewidth=2)
plt.title('Root Mean Squared Error (RMSE) per Epoch', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('RMSE', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

# Menampilkan nilai RMSE akhir untuk Train dan Validation
final_rmse_train = history.history['root_mean_squared_error'][-1]
final_rmse_val = history.history['val_root_mean_squared_error'][-1]

print(f"Final RMSE (Train): {final_rmse_train:.4f}")
print(f"Final RMSE (Validation): {final_rmse_val:.4f}")

"""Model Collaborative Filtering menunjukkan performa yang baik dengan nilai RMSE akhir sebesar 0.1780 pada data pelatihan (Train), yang mengindikasikan bahwa model mampu memprediksi rating film dengan rata-rata kesalahan sekitar 0.18 poin. Ini menunjukkan bahwa model telah berhasil mempelajari pola dari data training dengan cukup akurat. Sementara itu, pada data validasi, model mencatat nilai RMSE sebesar 0.1999, hanya sedikit lebih tinggi dari data pelatihan, yang menandakan kemampuan generalisasi model yang baik serta minimnya overfitting. Secara keseluruhan, RMSE di bawah 0.3 pada skala rating 1â€“5 membuktikan bahwa model cukup akurat dan andal dalam merekomendasikan film yang sesuai dengan preferensi pengguna."""